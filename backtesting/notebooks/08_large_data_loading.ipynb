{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Data Loading Example\n",
    "\n",
    "This notebook demonstrates how to use the `LargeDataLoader` to:\n",
    "1. Load subsets from large institutional datasets (5000+ securities)\n",
    "2. Apply corporate action adjustments (splits, dividends)\n",
    "3. Handle time-varying sector classifications\n",
    "4. Convert data to backtester format\n",
    "5. Run a backtest with the loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from backtesting import (\n",
    "    LargeDataLoader,\n",
    "    convert_to_backtester_format,\n",
    "    Backtester,\n",
    "    BacktestConfig,\n",
    "    DataManager,\n",
    "    SignalBasedTradeGenerator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Sample Large Data Files\n",
    "\n",
    "In practice, you would have these files from your data provider. Here we create synthetic data for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data directory\n",
    "large_data_dir = Path('../data/large_data')\n",
    "large_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create a universe of 100 securities (simulating 5000+ in production)\n",
    "all_tickers = [f'TICK{i:04d}' for i in range(100)]\n",
    "dates = pd.date_range('2023-01-01', '2024-12-31', freq='B')\n",
    "\n",
    "print(f\"Created universe of {len(all_tickers)} tickers\")\n",
    "print(f\"Date range: {dates[0]} to {dates[-1]} ({len(dates)} trading days)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic price data\n",
    "np.random.seed(42)\n",
    "price_data = []\n",
    "\n",
    "for ticker in all_tickers:\n",
    "    # Random walk for prices\n",
    "    returns = np.random.normal(0.0005, 0.02, len(dates))\n",
    "    prices = 100 * np.exp(np.cumsum(returns))\n",
    "    \n",
    "    for date, price in zip(dates, prices):\n",
    "        price_data.append({\n",
    "            'date': date,\n",
    "            'ticker': ticker,\n",
    "            'price': price\n",
    "        })\n",
    "\n",
    "prices_df = pd.DataFrame(price_data)\n",
    "print(f\"Created price data: {len(prices_df)} rows\")\n",
    "\n",
    "# Save as parquet (recommended for large files)\n",
    "prices_df.to_parquet(large_data_dir / 'prices_large.parquet', index=False)\n",
    "print(f\"Saved to {large_data_dir / 'prices_large.parquet'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate price adjustments for corporate actions\n",
    "# Simulate splits and dividends for a subset of securities\n",
    "adjustment_data = []\n",
    "\n",
    "for i, ticker in enumerate(all_tickers[:20]):  # Apply to first 20 tickers\n",
    "    # 2-for-1 split on random date\n",
    "    split_date = np.random.choice(dates[len(dates)//2:])\n",
    "    adjustment_data.append({\n",
    "        'date': split_date,\n",
    "        'ticker': ticker,\n",
    "        'adjustment_factor': 0.5,  # 2-for-1 split\n",
    "        'event_type': 'split'\n",
    "    })\n",
    "\n",
    "adjustments_df = pd.DataFrame(adjustment_data)\n",
    "print(f\"Created {len(adjustments_df)} price adjustments\")\n",
    "adjustments_df.to_parquet(large_data_dir / 'price_adjustments.parquet', index=False)\n",
    "print(f\"Saved to {large_data_dir / 'price_adjustments.parquet'}\")\n",
    "\n",
    "print(\"\\nSample adjustments:\")\n",
    "print(adjustments_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ADV (Average Daily Volume) data\n",
    "adv_data = []\n",
    "\n",
    "for ticker in all_tickers:\n",
    "    base_adv = np.random.uniform(500_000, 10_000_000)\n",
    "    \n",
    "    for date in dates:\n",
    "        # Add some time variation\n",
    "        daily_adv = base_adv * np.random.uniform(0.8, 1.2)\n",
    "        adv_data.append({\n",
    "            'date': date,\n",
    "            'ticker': ticker,\n",
    "            'adv': daily_adv\n",
    "        })\n",
    "\n",
    "adv_df = pd.DataFrame(adv_data)\n",
    "adv_df.to_parquet(large_data_dir / 'adv_large.parquet', index=False)\n",
    "print(f\"Created ADV data: {len(adv_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate beta data\n",
    "beta_data = []\n",
    "\n",
    "for ticker in all_tickers:\n",
    "    base_beta = np.random.uniform(0.5, 1.5)\n",
    "    \n",
    "    for date in dates:\n",
    "        # Slowly varying beta\n",
    "        daily_beta = base_beta + np.random.normal(0, 0.05)\n",
    "        beta_data.append({\n",
    "            'date': date,\n",
    "            'ticker': ticker,\n",
    "            'beta': daily_beta\n",
    "        })\n",
    "\n",
    "beta_df = pd.DataFrame(beta_data)\n",
    "beta_df.to_parquet(large_data_dir / 'betas_large.parquet', index=False)\n",
    "print(f\"Created beta data: {len(beta_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate time-varying sector mapping\n",
    "sectors = ['Technology', 'Healthcare', 'Financials', 'Energy', 'Consumer Discretionary']\n",
    "sector_data = []\n",
    "\n",
    "for ticker in all_tickers:\n",
    "    # Initial sector\n",
    "    current_sector = np.random.choice(sectors)\n",
    "    sector_data.append({\n",
    "        'date': dates[0],\n",
    "        'ticker': ticker,\n",
    "        'sector': current_sector\n",
    "    })\n",
    "    \n",
    "    # Some tickers change sectors mid-period\n",
    "    if np.random.random() < 0.1:  # 10% change sectors\n",
    "        change_date = np.random.choice(dates[len(dates)//2:])\n",
    "        new_sector = np.random.choice([s for s in sectors if s != current_sector])\n",
    "        sector_data.append({\n",
    "            'date': change_date,\n",
    "            'ticker': ticker,\n",
    "            'sector': new_sector\n",
    "        })\n",
    "\n",
    "sector_df = pd.DataFrame(sector_data)\n",
    "sector_df.to_parquet(large_data_dir / 'sector_mapping_dated.parquet', index=False)\n",
    "print(f\"Created sector mapping: {len(sector_df)} entries\")\n",
    "print(f\"Tickers with sector changes: {sector_df.groupby('ticker').size().gt(1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Data for a Specific Universe and Date Range\n",
    "\n",
    "Now we'll use the `LargeDataLoader` to load data for a subset of securities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loader\n",
    "loader = LargeDataLoader(\n",
    "    data_dir=str(large_data_dir),\n",
    "    use_float32=True  # Save memory\n",
    ")\n",
    "\n",
    "# Define our backtest universe (subset of all securities)\n",
    "backtest_universe = all_tickers[:30]  # Use first 30 securities\n",
    "start_date = '2023-06-01'\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "print(f\"Loading data for {len(backtest_universe)} securities\")\n",
    "print(f\"Date range: {start_date} to {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prices with corporate action adjustments\n",
    "prices = loader.load_prices_with_adjustments(\n",
    "    universe=backtest_universe,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    prices_file='prices_large.parquet',\n",
    "    adjustments_file='price_adjustments.parquet',\n",
    "    apply_adjustments=True\n",
    ")\n",
    "\n",
    "print(f\"Loaded prices: {prices.shape}\")\n",
    "print(f\"Date range: {prices.index[0]} to {prices.index[-1]}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(prices.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ADV data\n",
    "adv = loader.load_adv(\n",
    "    universe=backtest_universe,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    adv_file='adv_large.parquet'\n",
    ")\n",
    "\n",
    "print(f\"Loaded ADV: {adv.shape}\")\n",
    "print(f\"\\nSample ADV values:\")\n",
    "print(adv.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load beta data\n",
    "betas = loader.load_betas(\n",
    "    universe=backtest_universe,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    beta_file='betas_large.parquet'\n",
    ")\n",
    "\n",
    "print(f\"Loaded betas: {betas.shape}\")\n",
    "print(f\"\\nSample beta values:\")\n",
    "print(betas.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sector mapping (time-varying)\n",
    "sector_mapping = loader.load_sector_mapping_with_dates(\n",
    "    universe=backtest_universe,\n",
    "    date=end_date,  # Get sectors as of end date\n",
    "    sector_file='sector_mapping_dated.parquet'\n",
    ")\n",
    "\n",
    "print(f\"Loaded sector mapping: {len(sector_mapping)} securities\")\n",
    "print(f\"\\nSector distribution:\")\n",
    "print(sector_mapping['sector'].value_counts())\n",
    "print(f\"\\nSample mappings:\")\n",
    "print(sector_mapping.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Verify Price Adjustments\n",
    "\n",
    "Let's verify that corporate action adjustments were applied correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any of our universe had adjustments\n",
    "universe_adjustments = adjustments_df[adjustments_df['ticker'].isin(backtest_universe)]\n",
    "\n",
    "if len(universe_adjustments) > 0:\n",
    "    print(f\"Found {len(universe_adjustments)} adjustments in our universe:\")\n",
    "    print(universe_adjustments)\n",
    "    \n",
    "    # Pick one ticker with adjustment\n",
    "    example_ticker = universe_adjustments.iloc[0]['ticker']\n",
    "    adj_date = universe_adjustments.iloc[0]['date']\n",
    "    adj_factor = universe_adjustments.iloc[0]['adjustment_factor']\n",
    "    \n",
    "    print(f\"\\nExample: {example_ticker} had {adj_factor}x adjustment on {adj_date}\")\n",
    "    print(f\"This means prices BEFORE {adj_date} were multiplied by {adj_factor}\")\n",
    "    \n",
    "    # Show prices around adjustment date\n",
    "    if example_ticker in prices.columns:\n",
    "        ticker_prices = prices[example_ticker].dropna()\n",
    "        adj_date_pd = pd.Timestamp(adj_date)\n",
    "        \n",
    "        if adj_date_pd in ticker_prices.index:\n",
    "            idx = ticker_prices.index.get_loc(adj_date_pd)\n",
    "            window = ticker_prices.iloc[max(0, idx-5):min(len(ticker_prices), idx+6)]\n",
    "            print(f\"\\nPrices around adjustment date:\")\n",
    "            print(window)\n",
    "else:\n",
    "    print(\"No adjustments in our selected universe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Convert to Backtester Format\n",
    "\n",
    "Convert the loaded data to CSV format compatible with the backtester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to backtester format\n",
    "output_dir = Path('../data/converted_backtest_data')\n",
    "\n",
    "file_paths = convert_to_backtester_format(\n",
    "    prices=prices,\n",
    "    adv=adv,\n",
    "    betas=betas,\n",
    "    sector_mapping=sector_mapping,\n",
    "    factor_exposures=None,  # Not using factor model in this example\n",
    "    output_dir=str(output_dir)\n",
    ")\n",
    "\n",
    "print(\"Converted data saved to:\")\n",
    "for data_type, path in file_paths.items():\n",
    "    print(f\"  {data_type}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Backtest with Loaded Data\n",
    "\n",
    "Now use the converted data in a backtest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using DataManager\n",
    "data_manager = DataManager(str(output_dir))\n",
    "\n",
    "# Verify data was loaded\n",
    "print(f\"Loaded prices: {data_manager.prices.shape}\")\n",
    "print(f\"Loaded ADV: {data_manager.adv.shape}\")\n",
    "print(f\"Loaded betas: {data_manager.betas.shape}\")\n",
    "print(f\"Loaded sector mapping: {len(data_manager.sector_mapping)} securities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple momentum signal\n",
    "def momentum_signal(prices_df, lookback=20):\n",
    "    \"\"\"Simple momentum signal: rank by returns.\"\"\"\n",
    "    returns = prices_df.pct_change(lookback)\n",
    "    latest_returns = returns.iloc[-1]\n",
    "    \n",
    "    # Rank returns (higher is better)\n",
    "    ranks = latest_returns.rank(pct=True)\n",
    "    \n",
    "    # Convert to z-scores\n",
    "    signals = (ranks - 0.5) * 2  # Scale to [-1, 1]\n",
    "    \n",
    "    return signals\n",
    "\n",
    "# Generate signal\n",
    "signal = momentum_signal(data_manager.prices, lookback=20)\n",
    "print(f\"Generated signal for {len(signal)} securities\")\n",
    "print(f\"\\nTop 5 signals:\")\n",
    "print(signal.nlargest(5))\n",
    "print(f\"\\nBottom 5 signals:\")\n",
    "print(signal.nsmallest(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure backtest\n",
    "config = BacktestConfig(\n",
    "    initial_capital=1_000_000,\n",
    "    max_position_size=0.15,  # 15% per position\n",
    "    max_adv_participation=0.10,  # 10% ADV limit\n",
    "    transaction_cost_bps=10,\n",
    "    enable_beta_hedge=True,\n",
    "    target_beta=0.0,\n",
    "    enable_sector_hedge=True,\n",
    "    sector_hedge_method='proportional'\n",
    ")\n",
    "\n",
    "# Create trade generator\n",
    "trade_gen = SignalBasedTradeGenerator(\n",
    "    signal=signal,\n",
    "    target_positions=15  # Long top 15\n",
    ")\n",
    "\n",
    "print(\"Backtest configuration:\")\n",
    "print(f\"  Initial capital: ${config.initial_capital:,.0f}\")\n",
    "print(f\"  Max position size: {config.max_position_size:.1%}\")\n",
    "print(f\"  Beta hedge: {config.enable_beta_hedge}\")\n",
    "print(f\"  Sector hedge: {config.enable_sector_hedge}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run backtest\n",
    "backtester = Backtester(config, data_manager)\n",
    "results = backtester.run(trade_gen, rebalance_frequency='weekly')\n",
    "\n",
    "print(\"\\nBacktest completed!\")\n",
    "print(f\"Total return: {results.total_return:.2%}\")\n",
    "print(f\"Sharpe ratio: {results.sharpe_ratio:.2f}\")\n",
    "print(f\"Max drawdown: {results.max_drawdown:.2%}\")\n",
    "print(f\"Total trades: {len(results.trades)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show portfolio evolution\n",
    "portfolio_values = [state.total_value for state in results.states]\n",
    "dates_list = [state.date for state in results.states]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dates_list, portfolio_values)\n",
    "plt.title('Portfolio Value Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Portfolio Value ($)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal portfolio value: ${portfolio_values[-1]:,.2f}\")\n",
    "print(f\"Initial capital: ${config.initial_capital:,.2f}\")\n",
    "print(f\"P&L: ${portfolio_values[-1] - config.initial_capital:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Performance Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze trades\n",
    "trades_df = pd.DataFrame([\n",
    "    {\n",
    "        'date': t.date,\n",
    "        'ticker': t.ticker,\n",
    "        'shares': t.shares,\n",
    "        'price': t.price,\n",
    "        'value': abs(t.shares * t.price),\n",
    "        'cost': t.transaction_cost\n",
    "    }\n",
    "    for t in results.trades\n",
    "])\n",
    "\n",
    "print(\"Trade Statistics:\")\n",
    "print(f\"  Total trades: {len(trades_df)}\")\n",
    "print(f\"  Total value traded: ${trades_df['value'].sum():,.2f}\")\n",
    "print(f\"  Total transaction costs: ${trades_df['cost'].sum():,.2f}\")\n",
    "print(f\"  Average cost per trade: ${trades_df['cost'].mean():.2f}\")\n",
    "\n",
    "print(f\"\\nMost traded securities:\")\n",
    "print(trades_df['ticker'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete workflow for using the `LargeDataLoader`:\n",
    "\n",
    "1. **Created sample large data files** in Parquet format\n",
    "2. **Loaded subsets** for a specific universe and date range\n",
    "3. **Applied corporate action adjustments** backward from adjustment date\n",
    "4. **Loaded time-varying sector classifications**\n",
    "5. **Converted to backtester format** (CSV files)\n",
    "6. **Ran a backtest** with the loaded data\n",
    "\n",
    "### Key Advantages\n",
    "\n",
    "- **Memory efficient**: Only loads needed securities and dates\n",
    "- **Fast**: Parquet format is 10x faster than CSV\n",
    "- **Accurate**: Corporate actions applied correctly\n",
    "- **Flexible**: Supports time-varying classifications\n",
    "- **Reusable**: Converted data can be used in multiple backtests\n",
    "\n",
    "### Production Usage\n",
    "\n",
    "In production with 5000+ securities:\n",
    "- Store data in Parquet format\n",
    "- Use `use_float32=True` to save memory\n",
    "- Load only the universe and date range needed\n",
    "- Cache converted data for reuse\n",
    "- Consider partitioning by date for even faster loading"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
